{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R3beAM/Laboratorio-2-Modelos-de-lenguaje/blob/main/Lab2_ModelosLenguaje.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6142099",
      "metadata": {
        "id": "c6142099"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### Activity 2: Exploring Word Embeddings with GloVe and Numpy\n",
        "<br>\n",
        "\n",
        "- Objective:\n",
        "    - To understand the concept of word embeddings and their significance in Natural Language Processing.\n",
        "    - To learn how to manipulate and visualize high-dimensional data using dimensionality reduction techniques like PCA and t-SNE.\n",
        "    - To gain hands-on experience in implementing word similarity and analogies using GloVe embeddings and Numpy.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Instructions:\n",
        "    - Download GloVe pre-trained vectors from the provided link in Canvas, the official public project:\n",
        "    Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation\n",
        "    https://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "    - Create a dictorionay of the embeddings so that you carry out fast look ups. Save that dictionary e.g. as a serialized file for faster loading in future uses.\n",
        "    \n",
        "    - PCA and t-SNE Visualization: After loading the GloVe embeddings, use Numpy and Sklearn to perform PCA and t-SNE to reduce the dimensionality of the embeddings and visualize them in a 2D or 3D space.\n",
        "\n",
        "    - Word Similarity: Implement a function that takes a word as input and returns the 'n' most similar words based on their embeddings. You should use Numpy to implement this function, using libraries that already implement this function (e.g. Gensim) will result in zero points.\n",
        "\n",
        "    - Word Analogies: Implement a function to solve analogies between words. For example, \"man is to king as woman is to ____\". You should use Numpy to implement this function, using libraries that already implement this function (e.g. Gensim) will result in zero points.\n",
        "\n",
        "    - Submission: This activity is to be submitted in teams of 3 or 4. Only one person should submit the final work, with the full names of all team members included in a markdown cell at the beginning of the notebook.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Evaluation Criteria:\n",
        "\n",
        "    - Code Quality (40%): Your code should be well-organized, clearly commented, and easy to follow. Use also markdown cells for clarity.\n",
        "    \n",
        "   - Functionality (60%): All functions should work as intended, without errors.\n",
        "       - Visualization of PCA and t-SNE (10% each for a total of 20%)\n",
        "       - Similarity function (20%)\n",
        "       - Analogy function (20%)\n",
        "|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "329dd09a",
      "metadata": {
        "id": "329dd09a"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4af04b9d",
      "metadata": {
        "id": "4af04b9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a3ac03-be96-48f1-96f9-bbc9e9a72701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH = '/content/drive/MyDrive/glove.6B.50d.txt'\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8712cf42",
      "metadata": {
        "id": "8712cf42"
      },
      "source": [
        "#### Load file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8c3c9d2a",
      "metadata": {
        "id": "8c3c9d2a"
      },
      "outputs": [],
      "source": [
        "# PATH = '/media/pepe/DataUbuntu/Databases/glove_embeddings/glove.6B.200d.txt'\n",
        "PATH = '/content/drive/MyDrive/glove.6B.50d.txt'\n",
        "emb_dim = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "25a1eae6",
      "metadata": {
        "code_folding": [],
        "id": "25a1eae6"
      },
      "outputs": [],
      "source": [
        "# Create dictionary with embeddings\n",
        "def create_emb_dictionary(path=\"/content/drive/MyDrive/glove.6B.50d.txt\"):\n",
        "\n",
        "    embeddings = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if not parts:\n",
        "                continue\n",
        "            word, *vector = parts\n",
        "            values = np.asarray(vector, dtype=np.float32)\n",
        "            embeddings[word] = torch.from_numpy(values)\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "347a0e5e",
      "metadata": {
        "code_folding": [],
        "id": "347a0e5e"
      },
      "outputs": [],
      "source": [
        "# create dictionary\n",
        "embeddings_dict = create_emb_dictionary(\"/content/drive/MyDrive/glove.6B.50d.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1f01b760",
      "metadata": {
        "code_folding": [
          1
        ],
        "id": "1f01b760"
      },
      "outputs": [],
      "source": [
        "# Serialize\n",
        "with open('embeddings_dict_50D.pkl', 'wb') as f:\n",
        "    pickle.dump(embeddings_dict, f)\n",
        "\n",
        "# Deserialize\n",
        "# with open('embeddings_dict_200D.pkl', 'rb') as f:\n",
        "#     embeddings_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0ab9aa",
      "metadata": {
        "id": "fd0ab9aa"
      },
      "source": [
        "#### See some embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8b0991a9",
      "metadata": {
        "id": "8b0991a9"
      },
      "outputs": [],
      "source": [
        "# Show some\n",
        "def show_n_first_words(path, n_words):\n",
        "        with open(path, 'r') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                print(line.split(), len(line.split()[1:]))\n",
        "                if i>=n_words: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a16259ec",
      "metadata": {
        "id": "a16259ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "1f18199e-774a-47dc-c504-1bf2ba30d168"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/media/pepe/DataUbuntu/Databases/glove_embeddings/glove.6B.50d.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2370738075.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_n_first_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1280501104.py\u001b[0m in \u001b[0;36mshow_n_first_words\u001b[0;34m(path, n_words)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Show some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_n_first_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/pepe/DataUbuntu/Databases/glove_embeddings/glove.6B.50d.txt'"
          ]
        }
      ],
      "source": [
        "show_n_first_words(PATH, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff828123",
      "metadata": {
        "id": "ff828123"
      },
      "source": [
        "### Plot some embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "3d3ffaa4",
      "metadata": {
        "id": "3d3ffaa4"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Add PCA and t-SNE visualization\n",
        "\n",
        "def plot_embeddings(emb_path, words2show, emb_dim, embeddings_dict, func=PCA, n_components=2, random_state=42):\n",
        "    \"\"\"Reduce the dimensionality of selected embeddings and plot them.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    emb_path : str\n",
        "        Path to the embeddings file (kept for backwards compatibility).\n",
        "    words2show : Iterable[str]\n",
        "        Collection of tokens to visualise.\n",
        "    emb_dim : int\n",
        "        Dimensionality of the embeddings (unused but kept for clarity).\n",
        "    embeddings_dict : Dict[str, torch.Tensor]\n",
        "        Dictionary mapping tokens to their embedding vectors.\n",
        "    func : Callable\n",
        "        A dimensionality reduction class from sklearn (e.g. PCA or TSNE).\n",
        "    n_components : int, optional\n",
        "        Number of dimensions to project to. Defaults to 2.\n",
        "    random_state : int, optional\n",
        "        Random seed used by stochastic reducers such as t-SNE.\n",
        "    \"\"\"\n",
        "    import inspect\n",
        "\n",
        "    vectors = []\n",
        "    labels = []\n",
        "    for word in words2show:\n",
        "        vector = embeddings_dict.get(word)\n",
        "        if vector is None:\n",
        "            print(f\"Word '{word}' not found in the embeddings dictionary.\")\n",
        "            continue\n",
        "        # ensure numpy array\n",
        "        vectors.append(vector.cpu().numpy())\n",
        "        labels.append(word)\n",
        "\n",
        "    if not vectors:\n",
        "        raise ValueError(\"No valid words were provided to plot.\")\n",
        "\n",
        "    matrix = np.vstack(vectors)\n",
        "\n",
        "    reducer_kwargs = {\"n_components\": n_components}\n",
        "    if func is TSNE:\n",
        "        # perplexity must be less than the number of samples\n",
        "        max_perplexity = max(1, len(labels) - 1)\n",
        "        perplexity = min(30, max_perplexity)\n",
        "        if perplexity < 5 and max_perplexity >= 5:\n",
        "            perplexity = max_perplexity\n",
        "        reducer_kwargs.update({\n",
        "            \"random_state\": random_state,\n",
        "            \"init\": \"pca\",\n",
        "            \"learning_rate\": \"auto\",\n",
        "            \"perplexity\": perplexity\n",
        "        })\n",
        "    else:\n",
        "        params = inspect.signature(func.__init__).parameters\n",
        "        if \"random_state\" in params:\n",
        "            reducer_kwargs[\"random_state\"] = random_state\n",
        "\n",
        "    reducer = func(**reducer_kwargs)\n",
        "    reduced = reducer.fit_transform(matrix)\n",
        "\n",
        "    if n_components not in (2, 3):\n",
        "        raise ValueError(\"Only 2D or 3D plots are supported.\")\n",
        "\n",
        "    if n_components == 2:\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        ax.scatter(reduced[:, 0], reduced[:, 1], alpha=0.7)\n",
        "        for label, x, y in zip(labels, reduced[:, 0], reduced[:, 1]):\n",
        "            ax.annotate(label, (x, y))\n",
        "        ax.set_xlabel(\"Component 1\")\n",
        "        ax.set_ylabel(\"Component 2\")\n",
        "    else:\n",
        "        from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 - imported for side effects\n",
        "        fig = plt.figure(figsize=(12, 8))\n",
        "        ax = fig.add_subplot(111, projection=\"3d\")\n",
        "        ax.scatter(reduced[:, 0], reduced[:, 1], reduced[:, 2], alpha=0.7)\n",
        "        for label, x, y, z in zip(labels, reduced[:, 0], reduced[:, 1], reduced[:, 2]):\n",
        "            ax.text(x, y, z, label)\n",
        "        ax.set_xlabel(\"Component 1\")\n",
        "        ax.set_ylabel(\"Component 2\")\n",
        "        ax.set_zlabel(\"Component 3\")\n",
        "\n",
        "    ax.set_title(f\"{func.__name__} projection of GloVe embeddings\")\n",
        "    plt.show()\n",
        "\n",
        "    return {\"labels\": labels, \"embeddings\": reduced}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1b120a",
      "metadata": {
        "id": "9b1b120a"
      },
      "outputs": [],
      "source": [
        "words= ['burger', 'tortilla', 'bread', 'pizza', 'beef', 'steak', 'fries', 'chips',\n",
        "            'argentina', 'mexico', 'spain', 'usa', 'france', 'italy', 'greece', 'china',\n",
        "            'water', 'beer', 'tequila', 'wine', 'whisky', 'brandy', 'vodka', 'coffee', 'tea',\n",
        "            'apple', 'banana', 'orange', 'lemon', 'grapefruit', 'grape', 'strawberry', 'raspberry',\n",
        "            'school', 'work', 'university', 'highschool']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d33789",
      "metadata": {
        "id": "87d33789"
      },
      "outputs": [],
      "source": [
        "#\n",
        "plot_embeddings(PATH, words, emb_dim, embeddings_dict, PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22bacb4",
      "metadata": {
        "id": "f22bacb4"
      },
      "outputs": [],
      "source": [
        "# t-SNE dimensionality reduction for visualization\n",
        "embeddings = plot_embeddings(PATH, words, emb_dim, embeddings_dict, tSNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8211b7f1",
      "metadata": {
        "id": "8211b7f1"
      },
      "source": [
        "### Let us compute analogies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0003ce18",
      "metadata": {
        "id": "0003ce18"
      },
      "outputs": [],
      "source": [
        "# analogy\n",
        "def analogy(word1, word2, word3, embeddings_dict):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c02ec01",
      "metadata": {
        "id": "9c02ec01"
      },
      "outputs": [],
      "source": [
        "analogy('man', 'king', 'woman', embeddings_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb50e0f7",
      "metadata": {
        "id": "bb50e0f7"
      },
      "outputs": [],
      "source": [
        "# most similar\n",
        "def find_most_similar(word, embeddings_dict, top_n=10):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7946d712",
      "metadata": {
        "id": "7946d712"
      },
      "outputs": [],
      "source": [
        "most_similar = find_most_similar('mexico', embeddings_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b6e0ea1",
      "metadata": {
        "id": "3b6e0ea1"
      },
      "outputs": [],
      "source": [
        "for i, w in enumerate(most_similar, 1):\n",
        "    print(f'{i} ---> {w[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09875cae",
      "metadata": {
        "id": "09875cae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "985bfda4",
      "metadata": {
        "id": "985bfda4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06f33c5",
      "metadata": {
        "id": "f06f33c5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05e86fa7",
      "metadata": {
        "id": "05e86fa7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e1838b",
      "metadata": {
        "id": "e1e1838b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe1a9fad",
      "metadata": {
        "id": "fe1a9fad"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}